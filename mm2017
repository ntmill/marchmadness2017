###################################################
# step 1 - import libraries and data
###################################################

import os
import math
import pandas as pd
import numpy as np
import seaborn as sb
import sklearn as sk
import statsmodels.formula.api as smf
from sklearn import preprocessing
from sklearn import decomposition
from sklearn import metrics
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LinearRegression
from sklearn.metrics import roc_auc_score
from sklearn.grid_search import GridSearchCV
from sklearn.cross_validation import cross_val_score
from matplotlib import pyplot as plt

os.chdir('/Users/ntmill/OneDrive/Data/March Madness/2016')
df = pd.read_csv('training_export.csv')
target = df.pop('team1_victory')
headers = list(df.loc[:,:])
vars = list(df.loc[:,'team1_win':'team2_seed'])

###################################################
# step 2 - transform variables
###################################################

min_max_scaler = preprocessing.MinMaxScaler(feature_range=(0,1))
df.loc[:,vars] = min_max_scaler.fit_transform(df.loc[:,vars])

###################################################
# step 3 - test train split
###################################################

xtrain, xtest, ytrain, ytest = sk.cross_validation.train_test_split(
    df.values, target.values, train_size=0.66, random_state=201)

xtrain = pd.DataFrame(xtrain)
xtrain.columns = [headers]
xtest = pd.DataFrame(xtest)
xtest.columns = [headers]
xtrain_vars = xtrain.iloc[:, 8:]
xtest_vars = xtest.iloc[:, 8:]
ytrain = pd.DataFrame(ytrain)
ytest = pd.DataFrame(ytest)

###################################################
# step 4 - pca
###################################################

pca = decomposition.PCA(n_components=15)
pca_fit = pca.fit(df.loc[:,vars])
decomposition.PCA(copy=True, n_components=15, whiten=False)
existing_2d = pca.transform(df.loc[:,vars])
existing_2d = pd.DataFrame(existing_2d)
existing_2d.index = df.loc[:,vars].index
existing_2d.columns = ['PC-1','PC-2','PC-3','PC-4','PC-5','PC-6','PC-7','PC-8','PC-9','PC-10','PC-11','PC-12','PC-13','PC-14','PC-15']

# cume var plot
var = pca_fit.explained_variance_ratio_
var1 = np.cumsum(np.round(pca.explained_variance_ratio_, decimals=4)*100)
plt.plot(var1)

###################################################
# step 5 - logistic regression
###################################################

lr = sk.linear_model.LogisticRegression(C=1, random_state=201)
lr_train = lr.fit(xtrain_vars, ytrain)
lr_valid_prob_all = pd.DataFrame(lr_train.predict_proba(xtest_vars))
lr_valid_prob = lr_valid_prob_all[1]
math.sqrt(metrics.mean_squared_error(ytest,lr_valid_prob))


###################################################
# step 6 - random forest
###################################################

rf = RandomForestClassifier(n_estimators=500, max_features=2, min_samples_leaf=5, random_state=201, n_jobs=-1)
rf_train = rf.fit(xtrain_vars, ytrain)
rf_valid_prob_all = pd.DataFrame(rf.predict_proba(xtest_vars))
rf_valid_prob = rf_valid_prob_all[1]

importance = rf.feature_importances_
importance = pd.DataFrame(importance, index=pd.DataFrame(xtest_vars).columns, 
                          columns=["Importance"])

importance["Std"] = np.std([tree.feature_importances_
                            for tree in rf.estimators_], axis=0)
importance
pd.DataFrame(xtest).head()
pd.DataFrame(rf_train.feature_importances_, index=pd.DataFrame(xtrain_vars).columns)
